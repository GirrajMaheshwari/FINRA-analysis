{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import nltk\n",
    "import nltk.tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import string\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#listing all claim type to search\n",
    "claim_type = [\"breach of fiduciary duties\", \"breach  of  contract\", \"Murder\"]\n",
    "\n",
    "\n",
    "# load the files\n",
    "dict  = {}\n",
    "text = \"\"\n",
    "src = \"C://Users//acer1//Desktop//Course content//Legal analytics//Legal Analytics Project//Corpus\"\n",
    "for files in os.listdir(src)[0:200]:\n",
    "    Document_text = open(\"C://Users//acer1//Desktop//Course content//Legal analytics//Legal Analytics Project//Corpus//%s\"%files)\n",
    "    Data = Document_text.read()\n",
    "    \n",
    "    # Text after case summary - first para\n",
    "    if \"CASE SUMMARY\" in Data:\n",
    "        case_sum = Data.split(\"CASE SUMMARY\")[1].lower()\n",
    "        #taking first paragraph\n",
    "        case_sum_firpar = case_sum.split(\"\\n\\n\")[1]\n",
    "        #removing punctuation\n",
    "        case_sum_firpar = case_sum_firpar.translate(None, string.punctuation)\n",
    "        #stemming the text\n",
    "        case_sum_stem =  \" \".join([ps.stem(y.decode(\"utf8\",\"ignore\")) for y in case_sum_firpar.split()])\n",
    "        lst = [(word in case_sum_stem) for word in claim_type]\n",
    "        dict[files] = lst    \n",
    "\n",
    "#print dict  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#n_fin = nltk.ngrams(text.split(),3)\n",
    "#counter_uni_fin = Counter(n_fin)\n",
    "#counter_uni_fin.most_common(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to be done\n",
    "#put all claim type in claim_type list with stem words\n",
    "#data frame to put all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named spacy",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-c080f6458562>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named spacy"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
